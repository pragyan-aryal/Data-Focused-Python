{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_search_driver(job_title):\n",
    "\n",
    "    # Set up Chrome options in headless mode\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    \n",
    "    # HTML url encoding for search\n",
    "    job_title = job_title.replace(\" \", \"%20\")\n",
    "\n",
    "    search_url = \"https://www.usajobs.gov/Search/Results?k=\"+job_title\n",
    "\n",
    "    # Create a Chrome webdriver instance with the specified options\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        # Navigate to the URL\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Get the HTML content after dynamic loading\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Close the browser window\n",
    "        driver.quit()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all the titles within the specified class\n",
    "        page_listings = [li.a.get('title') for li in soup.find_all('li', class_='usajobs-search-pagination__page') if li.a]\n",
    "\n",
    "        # Get max pages\n",
    "        max_pages = int(page_listings[-1][-2:])\n",
    "\n",
    "    except Exception as e:\n",
    "        max_pages = 1\n",
    "        print(f\"Failed to retrieve search result from source page. Status code: {str(e)}\")\n",
    "\n",
    "    return max_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_search_driver(job_title,page_no):\n",
    "\n",
    "    # Set up Chrome options in headless mode\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    \n",
    "    # HTML url encoding for search\n",
    "    job_title = job_title.replace(\" \", \"%20\")\n",
    "\n",
    "    search_url = \"https://www.usajobs.gov/Search/Results?k=\"+job_title +'&p='+ str(page_no)\n",
    "\n",
    "    # Create a Chrome webdriver instance with the specified options\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        # Navigate to the URL\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Get the HTML content after dynamic loading\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Close the browser window\n",
    "        driver.quit()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    except Exception as e:\n",
    "        soup = BeautifulSoup()\n",
    "        print(f\"Failed to retrieve search result from source page. Status code: {str(e)}\")\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_extractor(soup):\n",
    "    \n",
    "    # Find all job listings\n",
    "    job_listings = soup.find_all('div', class_='usajobs-search-result--core')\n",
    "\n",
    "    # Initialize lists to store extracted information\n",
    "    job_titles = []\n",
    "    departments = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    salaries = []\n",
    "\n",
    "    # Loop through each job listing\n",
    "    for job_listing in job_listings:\n",
    "        job_titles.append(job_listing.find('a', class_='usajobs-search-result--core__title').text.strip())\n",
    "        departments.append(job_listing.find('h4', class_='usajobs-search-result--core__agency').text.strip())\n",
    "        companies.append(job_listing.find('h5', class_='usajobs-search-result--core__department').text.strip())\n",
    "        locations.append(job_listing.find('h4', class_='usajobs-search-result--core__location').text.strip())\n",
    "        salaries.append(job_listing.find('li', class_='usajobs-search-result--core__item').text.strip())\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'job title': job_titles,\n",
    "        'agency': departments,\n",
    "        'department': companies,\n",
    "        'location': locations,\n",
    "        'salary': salaries\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_usajobs(job_titles):\n",
    "    \n",
    "    # URL of the USAJOBS website\n",
    "    url = \"https://www.usajobs.gov/\"\n",
    "\n",
    "    # output_df = pd.DataFrame()\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for title in job_titles:\n",
    "            print(title+\" started\")\n",
    "            max_pages = page_search_driver(title)\n",
    "            print(str(max_pages)+\" pages found\")\n",
    "            for i in range(1,max_pages+1):\n",
    "                soup = job_search_driver(title,i)\n",
    "                df = info_extractor(soup)\n",
    "                df['search_keyword'] = title\n",
    "                df.to_csv(title+\"_\"+str(i)+\".csv\",index=False)\n",
    "                # output_df = pd.concat([output_df,df])\n",
    "                print(str(i)+\" page done\")\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve source page. Status code: {response.status_code}\")\n",
    "    \n",
    "    # return output_df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_usajobs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find job listings based on specific job titles\n",
    "job_titles = [#\"Data Analyst\", \n",
    "              #Data Engineer\", \n",
    "              # \"Business Analyst\", \n",
    "              # \"Product Manager\", \n",
    "              # \"Project Manager\", \n",
    "              # \"Mechanical Engineer\", \n",
    "              # \"Civil Engineer\", \n",
    "               \"Software Engineer\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_usajobs(job_titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
